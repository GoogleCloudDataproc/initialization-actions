#!/bin/bash
#
[% INSERT legal/license_header %]
#
[% PROCESS common/template_disclaimer %]
#
# This script installs NVIDIA GPU drivers (version 550.135) along with
# CUDA 12.4.
#
# Additionally, it installs the RAPIDS Spark plugin, configures Spark
# and YARN, installs an agent to collect GPU utilization metrics.  The
# installer is compatible with Debian, Ubuntu, and Rocky Linux
# distributions.
#
# Note that the script is designed to work both when secure boot is
# enabled with a custom image and when disabled during cluster
# creation.
#
# For details see
# github.com/GoogleCloudDataproc/custom-images/tree/main/examples/secure-boot
#

set -euxo pipefail

[% INSERT common/util_functions %]

[% INSERT gpu/util_functions %]

function main() {
  setup_gpu_yarn

  if [[ "${RAPIDS_RUNTIME}" == "SPARK" ]]; then
    install_spark_rapids
    configure_gpu_script
    echo "RAPIDS initialized with Spark runtime"
  elif [[ "${RAPIDS_RUNTIME}" == "DASK" ]]; then
    # we are not currently tooled for installing dask in this action.
    echo "RAPIDS recognizes DASK runtime - currently supported using dask/dask.sh or rapids/rapids.sh"
  else
    echo "Unrecognized RAPIDS Runtime: ${RAPIDS_RUNTIME}"
  fi

  # Restart YARN services if they are running already
  for svc in resourcemanager nodemanager; do
    if [[ "$(systemctl show hadoop-yarn-${svc}.service -p SubState --value)" == 'running' ]]; then
      systemctl restart "hadoop-yarn-${svc}.service"
    fi
  done
}

function exit_handler() {
  # Purge private key material until next grant
  clear_dkms_key

  set +ex
  echo "Exit handler invoked"

  # Clear pip cache
  pip cache purge || echo "unable to purge pip cache"

  # If system memory was sufficient to mount memory-backed filesystems
  if [[ "${tmpdir}" == "/mnt/shm" ]] ; then
    # remove the tmpfs pip cache-dir
    pip config unset global.cache-dir || echo "unable to unset global pip cache"

    # Clean up shared memory mounts
    for shmdir in /var/cache/apt/archives /var/cache/dnf /mnt/shm /tmp /var/cudnn-local ; do
      if ( grep -q "^tmpfs ${shmdir}" /proc/mounts && ! grep -q "^tmpfs ${shmdir}" /etc/fstab ) ; then
        umount -f ${shmdir}
      fi
    done

    # restart services stopped during preparation stage
    # systemctl list-units | perl -n -e 'qx(systemctl start $1) if /^.*? ((hadoop|knox|hive|mapred|yarn|hdfs)\S*).service/'
  fi

  if is_debuntu ; then
    # Clean up OS package cache
    apt-get -y -qq clean
    apt-get -y -qq -o DPkg::Lock::Timeout=60 autoremove
    # re-hold systemd package
    if ge_debian12 ; then
    apt-mark hold systemd libsystemd0 ; fi
    hold_nvidia_packages
  else
    dnf clean all
  fi

  # print disk usage statistics for large components
  if is_ubuntu ; then
    du -hs \
      /usr/lib/{pig,hive,hadoop,jvm,spark,google-cloud-sdk,x86_64-linux-gnu} \
      /usr/lib \
      /opt/nvidia/* \
      /usr/local/cuda-1?.? \
      /opt/conda/miniconda3 | sort -h
  elif is_debian ; then
    du -x -hs \
      /usr/lib/{pig,hive,hadoop,jvm,spark,google-cloud-sdk,x86_64-linux-gnu} \
      /var/lib/{docker,mysql,} \
      /usr/lib \
      /opt/nvidia/* \
      /usr/local/cuda-1?.? \
      /opt/{conda,google-cloud-ops-agent,install-nvidia,} \
      /usr/bin \
      /usr \
      /var \
      / 2>/dev/null | sort -h
  else
    du -hs \
      /var/lib/docker \
      /usr/lib/{pig,hive,hadoop,firmware,jvm,spark,atlas} \
      /usr/lib64/google-cloud-sdk \
      /usr/lib \
      /opt/nvidia/* \
      /usr/local/cuda-1?.? \
      /opt/conda/miniconda3
  fi

  # Process disk usage logs from installation period
  rm -f /run/keep-running-df
  sync
  sleep 5.01s
  # compute maximum size of disk during installation
  # Log file contains logs like the following (minus the preceeding #):
#Filesystem     1K-blocks    Used Available Use% Mounted on
#/dev/vda2        7096908 2611344   4182932  39% /
  df / | tee -a "/run/disk-usage.log"

  perl -e '@siz=( sort { $a => $b }
                   map { (split)[2] =~ /^(\d+)/ }
                  grep { m:^/: } <STDIN> );
$max=$siz[0]; $min=$siz[-1]; $inc=$max-$min;
print( "    samples-taken: ", scalar @siz, $/,
       "maximum-disk-used: $max", $/,
       "minimum-disk-used: $min", $/,
       "     increased-by: $inc", $/ )' < "/run/disk-usage.log"

  echo "exit_handler has completed"

  # zero free disk space
  if [[ -n "$(get_metadata_attribute creating-image)" ]]; then
    dd if=/dev/zero of=/zero
    sync
    sleep 3s
    rm -f /zero
  fi

  return 0
}

function prepare_to_install(){
  prepare_common_env
  prepare_gpu_env

  # Fetch instance roles and runtime
  readonly MASTER=$(/usr/share/google/get_metadata_value attributes/dataproc-master)
}

prepare_to_install

main
