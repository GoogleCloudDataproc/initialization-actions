#!/bin/bash
#
[% INSERT legal/license_header %]
#
[% PROCESS common/template_disclaimer %]
#
# This initialization action script will install rapids on a Dataproc
# cluster.

set -euxo pipefail

[% INSERT common/util_functions %]

[% INSERT gpu/util_functions %]

[% INSERT dask/util_functions %]

function main() {
  setup_gpu_yarn

  echo "yarn setup complete"

  if ( test -v CUDNN_VERSION && [[ -n "${CUDNN_VERSION}" ]] ) ; then
    install_nvidia_nccl
    install_nvidia_cudnn
  fi

  if [[ "${RAPIDS_RUNTIME}" == "SPARK" ]]; then
    echo "RAPIDS recognizes SPARK runtime - currently supported using gpu/install_gpu_driver.sh or spark-rapids/spark-rapids.sh"
    exit 1
  fi

  # Install Dask with RAPIDS
  install_dask_rapids

  # In "standalone" mode, Dask relies on a systemd unit to launch.
  # In "yarn" mode, it relies a config.yaml file.
  if [[ "${DASK_RUNTIME}" == "yarn" ]]; then
    # Create cuda accelerated Dask YARN config file
    configure_dask_yarn
  else
    # Create Dask service
    install_systemd_dask_service
    start_systemd_dask_service

    configure_knox_for_dask

    local DASK_CLOUD_LOGGING="$(get_metadata_attribute dask-cloud-logging 'false')"
    if [[ "${DASK_CLOUD_LOGGING}" == "true" ]]; then
      configure_fluentd_for_dask
    fi
  fi
}

function exit_handler() {
  gpu_exit_handler
  conda_exit_handler
  common_exit_handler
  return 0
}

function prepare_to_install(){
  prepare_common_env
  conda_env="$(get_metadata_attribute conda-env 'dask-rapids')"
  readonly conda_env
  prepare_dask_rapids_env
  prepare_conda_env
  prepare_gpu_env
  trap exit_handler EXIT
}

prepare_to_install

main
